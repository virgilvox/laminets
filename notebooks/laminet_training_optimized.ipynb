{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Laminet Training Notebook (Optimized Version)\n",
        "\n",
        "This notebook implements the Laminet (Lamina Networks) architecture as described in the paper \"Lamina Networks: Emergent Semantic Reasoning via Evolving Memory Fields\". \n",
        "\n",
        "This is an **optimized version** designed to train in 30-60 minutes on a T4 GPU while still using the full dataset. Optimizations include:\n",
        "- Reduced model complexity (smaller field dimension, fewer attractor points, fewer evolution steps)\n",
        "- Mixed precision training\n",
        "- Optimized field evolution calculations\n",
        "- Larger batch size\n",
        "- Fewer training epochs\n",
        "\n",
        "## Setup and Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install required dependencies\n",
        "!pip install -q torch torchvision matplotlib numpy tqdm scikit-learn ipywidgets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm.notebook import tqdm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "from sklearn.manifold import TSNE\n",
        "import random\n",
        "import math\n",
        "from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
        "    print(f\"Memory Cached: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Laminet Model Architecture (Optimized)\n",
        "\n",
        "The Laminet model consists of several key components:\n",
        "1. **Encoder**: Maps text inputs to embedding space\n",
        "2. **Field Points**: Latent particles with position, velocity, mass, and charge\n",
        "3. **Evolution Engine**: Evolves field points under semantic forces (optimized for speed)\n",
        "4. **Decoder**: Translates evolved embeddings back to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class FieldPoint(nn.Module):\n",
        "    \"\"\"Represents a point in the semantic field with position, velocity, mass, and charge.\"\"\"\n",
        "    def __init__(self, embed_dim, init_position=None, init_mass=1.0, init_charge=0.0, init_decay=0.1):\n",
        "        super().__init__()\n",
        "        # Initialize position or use provided position\n",
        "        if init_position is not None:\n",
        "            self.position = nn.Parameter(init_position.clone().detach())\n",
        "        else:\n",
        "            self.position = nn.Parameter(torch.randn(embed_dim) * 0.02)\n",
        "            \n",
        "        # Initialize velocity with zeros\n",
        "        self.velocity = nn.Parameter(torch.zeros(embed_dim))\n",
        "        \n",
        "        # Mass and charge parameters\n",
        "        self.log_mass = nn.Parameter(torch.tensor(math.log(init_mass)))\n",
        "        self.charge = nn.Parameter(torch.tensor(init_charge))\n",
        "        self.decay = nn.Parameter(torch.tensor(init_decay))\n",
        "        \n",
        "    @property\n",
        "    def mass(self):\n",
        "        # Mass is always positive\n",
        "        return torch.exp(self.log_mass)\n",
        "    \n",
        "    def reset_velocity(self):\n",
        "        # Reset velocity to zero (useful between batches)\n",
        "        with torch.no_grad():\n",
        "            self.velocity.zero_()\n",
        "            \n",
        "    def __repr__(self):\n",
        "        return f\"FieldPoint(pos={self.position.norm():.2f}, vel={self.velocity.norm():.2f}, mass={self.mass.item():.2f}, charge={self.charge.item():.2f})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class EvolutionEngine(nn.Module):\n",
        "    \"\"\"Optimized engine that evolves field points based on semantic forces.\"\"\"\n",
        "    def __init__(self, epsilon=1e-6, min_distance=0.1, max_force=10.0):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon  # Prevent division by zero\n",
        "        self.min_distance = min_distance  # Minimum distance to prevent excessive forces\n",
        "        self.max_force = max_force  # Maximum force magnitude\n",
        "        \n",
        "    def compute_forces(self, positions, charges, masses):\n",
        "        \"\"\"Optimized force computation with batch operations.\"\"\"\n",
        "        n_points = positions.shape[0]\n",
        "        \n",
        "        # Vectorized operations for pairwise calculations\n",
        "        # Compute all distances at once\n",
        "        diffs = positions.unsqueeze(0) - positions.unsqueeze(1)  # [n, n, dim]\n",
        "        squared_dists = torch.sum(diffs**2, dim=-1)  # [n, n]\n",
        "        squared_dists = torch.clamp(squared_dists, min=self.min_distance**2) + self.epsilon\n",
        "        \n",
        "        # Compute charge products efficiently\n",
        "        charge_prods = charges.unsqueeze(0) * charges.unsqueeze(1)  # [n, n]\n",
        "        \n",
        "        # Compute force magnitudes\n",
        "        force_mags = charge_prods / squared_dists  # [n, n]\n",
        "        force_mags = torch.clamp(force_mags, min=-self.max_force, max=self.max_force)\n",
        "        \n",
        "        # Mask out self-interactions\n",
        "        mask = 1.0 - torch.eye(n_points, device=positions.device)\n",
        "        force_mags = force_mags * mask\n",
        "        \n",
        "        # Normalize directions and compute forces\n",
        "        dist = torch.sqrt(squared_dists).unsqueeze(-1)  # [n, n, 1]\n",
        "        norm_diffs = diffs / (dist + self.epsilon)  # [n, n, dim]\n",
        "        \n",
        "        # Apply forces\n",
        "        forces = torch.sum(norm_diffs * force_mags.unsqueeze(-1), dim=1)  # [n, dim]\n",
        "        \n",
        "        return forces\n",
        "        \n",
        "    def forward(self, field_points, delta_t=0.1, steps=5):  # Reduced steps for speed\n",
        "        \"\"\"Evolve field points over time.\"\"\"\n",
        "        # Extract field point properties\n",
        "        positions = torch.stack([p.position for p in field_points])\n",
        "        velocities = torch.stack([p.velocity for p in field_points])\n",
        "        masses = torch.stack([p.mass for p in field_points])\n",
        "        charges = torch.stack([p.charge for p in field_points])\n",
        "        decays = torch.stack([p.decay for p in field_points])\n",
        "        \n",
        "        # Store evolution history for visualization\n",
        "        position_history = [positions.clone().detach()]\n",
        "        \n",
        "        # Evolve the field for multiple steps\n",
        "        for step in range(steps):\n",
        "            # Compute forces\n",
        "            forces = self.compute_forces(positions, charges, masses)\n",
        "            \n",
        "            # Update velocities (F = ma -> a = F/m)\n",
        "            accelerations = forces / masses.unsqueeze(1)\n",
        "            \n",
        "            # Apply velocity decay (damping)\n",
        "            velocity_decay = (1.0 - decays * delta_t).unsqueeze(1)\n",
        "            velocities = velocity_decay * velocities + accelerations * delta_t\n",
        "            \n",
        "            # Update positions\n",
        "            positions = positions + velocities * delta_t\n",
        "            \n",
        "            # Store position history\n",
        "            position_history.append(positions.clone().detach())\n",
        "        \n",
        "        # Update field points with new positions and velocities\n",
        "        for i, point in enumerate(field_points):\n",
        "            point.position.data = positions[i].data\n",
        "            point.velocity.data = velocities[i].data\n",
        "        \n",
        "        # Calculate potential energy of the system (simplified for speed)\n",
        "        potential_energy = 0.0\n",
        "        n_points = len(field_points)\n",
        "        # Calculate potential energy for a subset of pairs\n",
        "        sample_rate = 0.5  # Only calculate half of all pairs\n",
        "        for i in range(n_points):\n",
        "            for j in range(i+1, n_points):\n",
        "                if random.random() < sample_rate:\n",
        "                    dist = torch.norm(field_points[i].position - field_points[j].position)\n",
        "                    potential_energy += (field_points[i].charge * field_points[j].charge) / (dist + self.epsilon)\n",
        "        potential_energy = potential_energy / sample_rate  # Scale to account for sampling\n",
        "        \n",
        "        return position_history, potential_energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class Laminet(nn.Module):\n",
        "    \"\"\"Optimized Laminet model that combines encoder, field evolution, and decoder.\"\"\"\n",
        "    def __init__(self, \n",
        "                 encoder_model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
        "                 field_dim=64,  # Reduced from 128 \n",
        "                 num_attractor_points=20,  # Reduced from 50\n",
        "                 num_evolution_steps=5,  # Reduced from 10\n",
        "                 delta_t=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder - use a pretrained sentence transformer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n",
        "        \n",
        "        # Get the encoder output dimension\n",
        "        self.embed_dim = self.encoder.config.hidden_size\n",
        "        \n",
        "        # Project encoder output to field space if dimensions don't match\n",
        "        self.field_dim = field_dim\n",
        "        if self.embed_dim != self.field_dim:\n",
        "            self.projector = nn.Linear(self.embed_dim, self.field_dim)\n",
        "        else:\n",
        "            self.projector = nn.Identity()\n",
        "        \n",
        "        # Memory field - attractor points in the field\n",
        "        self.attractor_points = nn.ModuleList([\n",
        "            FieldPoint(field_dim, init_charge=1.0) \n",
        "            for _ in range(num_attractor_points)\n",
        "        ])\n",
        "        \n",
        "        # Query point - created dynamically for each input\n",
        "        self.query_point = None\n",
        "        \n",
        "        # Evolution engine\n",
        "        self.evolution_engine = EvolutionEngine()\n",
        "        self.num_evolution_steps = num_evolution_steps\n",
        "        self.delta_t = delta_t\n",
        "        \n",
        "        # Decoder - transforms evolved field back to embedding space (simplified)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(field_dim, field_dim*2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(field_dim*2, field_dim),\n",
        "        )\n",
        "        \n",
        "        # Store the last field evolution for visualization\n",
        "        self.last_field_history = None\n",
        "        \n",
        "    def encode_text(self, texts):\n",
        "        \"\"\"Encode texts to embeddings.\"\"\"\n",
        "        # Tokenize texts\n",
        "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.encoder(**inputs)\n",
        "            # Use CLS token or mean pooling\n",
        "            embeddings = outputs.last_hidden_state[:, 0]  # CLS token\n",
        "            # Project to field dimension if needed\n",
        "            field_embeddings = self.projector(embeddings)\n",
        "            \n",
        "        return field_embeddings\n",
        "    \n",
        "    def create_query_point(self, embedding):\n",
        "        \"\"\"Create a query point from input embedding.\"\"\"\n",
        "        return FieldPoint(\n",
        "            self.field_dim,\n",
        "            init_position=embedding,\n",
        "            init_mass=0.5,  # Lower mass to be more influenced by attractors\n",
        "            init_charge=-1.0  # Opposite charge to be attracted to memory points\n",
        "        )\n",
        "    \n",
        "    def evolve_field(self, query_point):\n",
        "        \"\"\"Evolve the field with query and attractor points.\"\"\"\n",
        "        # Combine query and attractor points\n",
        "        all_points = [query_point] + list(self.attractor_points)\n",
        "        \n",
        "        # Evolve field\n",
        "        position_history, potential_energy = self.evolution_engine(\n",
        "            all_points, \n",
        "            delta_t=self.delta_t, \n",
        "            steps=self.num_evolution_steps\n",
        "        )\n",
        "        \n",
        "        # Store history for visualization\n",
        "        self.last_field_history = position_history\n",
        "        \n",
        "        # Return evolved query point position\n",
        "        return query_point.position, potential_energy\n",
        "    \n",
        "    def forward(self, source_texts):\n",
        "        \"\"\"Process input texts through the Laminet model.\"\"\"\n",
        "        # Encode source texts\n",
        "        source_embeddings = self.encode_text(source_texts)\n",
        "        \n",
        "        # Process each source embedding\n",
        "        evolved_embeddings = []\n",
        "        potential_energies = []\n",
        "        \n",
        "        for embedding in source_embeddings:\n",
        "            # Create query point\n",
        "            query_point = self.create_query_point(embedding)\n",
        "            \n",
        "            # Evolve field\n",
        "            evolved_embedding, potential_energy = self.evolve_field(query_point)\n",
        "            \n",
        "            evolved_embeddings.append(evolved_embedding)\n",
        "            potential_energies.append(potential_energy)\n",
        "            \n",
        "        # Stack evolved embeddings\n",
        "        evolved_embeddings = torch.stack(evolved_embeddings)\n",
        "        potential_energies = torch.stack(potential_energies)\n",
        "        \n",
        "        # Decode evolved embeddings\n",
        "        decoded_embeddings = self.decoder(evolved_embeddings)\n",
        "        \n",
        "        return decoded_embeddings, potential_energies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "The dataset contains 10,000 samples with source and target concepts from different semantic spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class LaminetDataset(Dataset):\n",
        "    \"\"\"Dataset for Laminet training.\"\"\"\n",
        "    def __init__(self, samples_path):\n",
        "        \"\"\"Initialize dataset from samples JSON file.\"\"\"\n",
        "        with open(samples_path, 'r') as f:\n",
        "            self.samples = json.load(f)\n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a sample by index.\"\"\"\n",
        "        sample = self.samples[idx]\n",
        "        return {\n",
        "            'sample_id': sample['sample_id'],\n",
        "            'source_text': sample['source_text'],\n",
        "            'target_text': sample['target_text'],\n",
        "            'source_space': sample['source_space'],\n",
        "            'source_concept': sample['source_concept'],\n",
        "            'target_space': sample['target_space'],\n",
        "            'target_concept': sample['target_concept'],\n",
        "            'transition_pattern': sample['transition_pattern']\n",
        "        }\n",
        "    \n",
        "    def get_spaces_and_concepts(self):\n",
        "        \"\"\"Get unique spaces and concepts for visualization.\"\"\"\n",
        "        spaces = set()\n",
        "        concepts = {}\n",
        "        \n",
        "        for sample in self.samples:\n",
        "            spaces.add(sample['source_space'])\n",
        "            spaces.add(sample['target_space'])\n",
        "            \n",
        "            source_space = sample['source_space']\n",
        "            target_space = sample['target_space']\n",
        "            source_concept = sample['source_concept']\n",
        "            target_concept = sample['target_concept']\n",
        "            \n",
        "            if source_space not in concepts:\n",
        "                concepts[source_space] = set()\n",
        "            if target_space not in concepts:\n",
        "                concepts[target_space] = set()\n",
        "                \n",
        "            concepts[source_space].add(source_concept)\n",
        "            concepts[target_space].add(target_concept)\n",
        "            \n",
        "        return spaces, concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Upload dataset to Colab if needed\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Check if dataset exists\n",
        "dataset_path = '/content/laminet_samples_10k.json'\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Please upload the dataset file:\")\n",
        "    uploaded = files.upload()\n",
        "    dataset_path = list(uploaded.keys())[0]\n",
        "    # If it's uploaded to a different path, move it to the expected path\n",
        "    if dataset_path != 'laminet_samples_10k.json':\n",
        "        !mv \"{dataset_path}\" \"/content/laminet_samples_10k.json\"\n",
        "        dataset_path = '/content/laminet_samples_10k.json'\n",
        "    print(f\"Dataset uploaded to {dataset_path}\")\n",
        "else:\n",
        "    print(f\"Dataset already exists at {dataset_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load the dataset\n",
        "dataset = LaminetDataset(dataset_path)\n",
        "\n",
        "# Split into train and validation sets (90/10 split)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Create data loaders with larger batch size\n",
        "batch_size = 64  # Increased from 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimized Model Training\n",
        "\n",
        "Initialize and train the Laminet model with mixed precision for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "field_dim = 64  # Reduced from 128\n",
        "num_attractor_points = 20  # Reduced from 50\n",
        "num_evolution_steps = 5  # Reduced from 10\n",
        "\n",
        "model = Laminet(\n",
        "    encoder_model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "    field_dim=field_dim,\n",
        "    num_attractor_points=num_attractor_points,\n",
        "    num_evolution_steps=num_evolution_steps\n",
        ").to(device)\n",
        "\n",
        "# Define loss functions\n",
        "cosine_loss = nn.CosineEmbeddingLoss()\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "# Define optimizer with learning rate scheduler\n",
        "learning_rate = 1e-3  # Increased from 5e-4\n",
        "optimizer = optim.AdamW(\n",
        "    [{'params': model.parameters(), 'lr': learning_rate}],\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    mode='min', \n",
        "    factor=0.5, \n",
        "    patience=1,  # Reduced from 2\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Initialize grad scaler for mixed precision training\n",
        "scaler = GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def train_epoch(model, train_loader, optimizer, epoch, scaler):\n",
        "    \"\"\"Train for one epoch with mixed precision.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "    \n",
        "    for batch in progress_bar:\n",
        "        # Get source and target texts\n",
        "        source_texts = batch['source_text']\n",
        "        target_texts = batch['target_text']\n",
        "        \n",
        "        # Reset optimizer\n",
        "        optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
        "        \n",
        "        # Forward pass with mixed precision\n",
        "        with autocast():\n",
        "            source_evolved, potential_energy = model(source_texts)\n",
        "            \n",
        "            # Get target embeddings\n",
        "            with torch.no_grad():\n",
        "                target_embeddings = model.encode_text(target_texts)\n",
        "            \n",
        "            # Compute cosine similarity loss\n",
        "            target_ones = torch.ones(source_evolved.size(0)).to(device)\n",
        "            cos_loss = cosine_loss(source_evolved, target_embeddings, target_ones)\n",
        "            \n",
        "            # Compute field coherence loss (regularization)\n",
        "            coherence_loss = torch.mean(potential_energy)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = cos_loss + 0.1 * coherence_loss\n",
        "        \n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        # Unscale before gradient clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        # Optimizer step with gradient scaling\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # Update statistics\n",
        "        total_loss += loss.item() * len(source_texts)\n",
        "        total_samples += len(source_texts)\n",
        "        \n",
        "        # Update progress bar\n",
        "        avg_loss = total_loss / total_samples\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f\"{avg_loss:.4f}\", \n",
        "            'cos_loss': f\"{cos_loss.item():.4f}\", \n",
        "            'coherence_loss': f\"{coherence_loss.item():.4f}\"\n",
        "        })\n",
        "        \n",
        "    return total_loss / total_samples\n",
        "\n",
        "def validate(model, val_loader, epoch):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    progress_bar = tqdm(val_loader, desc=f\"Validation {epoch}\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            # Get source and target texts\n",
        "            source_texts = batch['source_text']\n",
        "            target_texts = batch['target_text']\n",
        "            \n",
        "            # Forward pass\n",
        "            source_evolved, potential_energy = model(source_texts)\n",
        "            \n",
        "            # Get target embeddings\n",
        "            target_embeddings = model.encode_text(target_texts)\n",
        "            \n",
        "            # Compute cosine similarity loss\n",
        "            target_ones = torch.ones(source_evolved.size(0)).to(device)\n",
        "            cos_loss = cosine_loss(source_evolved, target_embeddings, target_ones)\n",
        "            \n",
        "            # Update statistics\n",
        "            total_loss += cos_loss.item() * len(source_texts)\n",
        "            total_samples += len(source_texts)\n",
        "            \n",
        "            # Update progress bar\n",
        "            avg_loss = total_loss / total_samples\n",
        "            progress_bar.set_postfix({'val_loss': f\"{avg_loss:.4f}\"})\n",
        "    \n",
        "    return total_loss / total_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Create directory for checkpoints\n",
        "os.makedirs('/content/checkpoints', exist_ok=True)\n",
        "\n",
        "# Training loop with fewer epochs\n",
        "num_epochs = 8  # Reduced from 20\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Enable cuDNN benchmark for faster training\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, epoch, scaler)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss = validate(model, val_loader, epoch)\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Save checkpoint if validation loss improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        checkpoint_path = f\"/content/checkpoints/laminet_optimized_epoch_{epoch}_loss_{val_loss:.4f}.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "    \n",
        "    # Report time elapsed\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Time elapsed: {elapsed/60:.2f} minutes\")\n",
        "    \n",
        "# Report total training time\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal training time: {total_time/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/loss_curve_optimized.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Save the final model\n",
        "final_model_path = \"/content/laminet_optimized_final.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'field_dim': field_dim,\n",
        "    'num_attractor_points': num_attractor_points,\n",
        "    'num_evolution_steps': num_evolution_steps,\n",
        "    'encoder_model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "}, final_model_path)\n",
        "print(f\"Saved final model to {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Field Visualization\n",
        "\n",
        "Visualize the semantic field evolution and memory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def visualize_field_static(model, sample_texts, title=\"Semantic Field Visualization\"):\n",
        "    \"\"\"Visualize the final state of the field after evolution.\"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Process sample texts\n",
        "    with torch.no_grad():\n",
        "        _, _ = model(sample_texts)\n",
        "    \n",
        "    # Get attractor points\n",
        "    attractor_positions = torch.stack([p.position.detach() for p in model.attractor_points])\n",
        "    \n",
        "    # Get query point from the last evolution step\n",
        "    field_history = model.last_field_history\n",
        "    if field_history is None or len(field_history) == 0:\n",
        "        print(\"No field history available. Run the model first.\")\n",
        "        return\n",
        "    \n",
        "    # Last position of query point (first point in the field)\n",
        "    final_positions = field_history[-1]\n",
        "    query_position = final_positions[0].unsqueeze(0)  # Add batch dimension\n",
        "    \n",
        "    # Combine query and attractor positions for t-SNE\n",
        "    all_positions = torch.cat([query_position, attractor_positions], dim=0)\n",
        "    all_positions_np = all_positions.cpu().numpy()\n",
        "    \n",
        "    # Apply t-SNE for dimensionality reduction\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(all_positions_np) - 1))\n",
        "    positions_2d = tsne.fit_transform(all_positions_np)\n",
        "    \n",
        "    # Plot the field\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    # Query point\n",
        "    plt.scatter(\n",
        "        positions_2d[0, 0], \n",
        "        positions_2d[0, 1], \n",
        "        color='red', \n",
        "        s=100, \n",
        "        marker='*', \n",
        "        label='Query'\n",
        "    )\n",
        "    \n",
        "    # Attractor points\n",
        "    plt.scatter(\n",
        "        positions_2d[1:, 0], \n",
        "        positions_2d[1:, 1], \n",
        "        color='blue', \n",
        "        s=50, \n",
        "        alpha=0.7, \n",
        "        label='Attractors'\n",
        "    )\n",
        "    \n",
        "    # Add labels to attractors\n",
        "    for i in range(len(attractor_positions)):\n",
        "        plt.text(positions_2d[i+1, 0], positions_2d[i+1, 1], f\"A{i}\", fontsize=8)\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/field_static_optimized.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def visualize_field_evolution(model, sample_text):\n",
        "    \"\"\"Visualize the evolution of field points over time.\"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Process sample text\n",
        "    with torch.no_grad():\n",
        "        _, _ = model([sample_text])\n",
        "    \n",
        "    # Get field history\n",
        "    field_history = model.last_field_history\n",
        "    if field_history is None or len(field_history) == 0:\n",
        "        print(\"No field history available. Run the model first.\")\n",
        "        return\n",
        "    \n",
        "    # Number of evolution steps\n",
        "    num_steps = len(field_history)\n",
        "    \n",
        "    # Apply t-SNE to the final state first (to create a consistent mapping)\n",
        "    final_positions = field_history[-1].cpu().numpy()\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(final_positions) - 1))\n",
        "    final_2d = tsne.fit_transform(final_positions)\n",
        "    \n",
        "    # Create a figure for animation\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    query_point = None\n",
        "    attractor_points = None\n",
        "    time_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
        "    \n",
        "    def init():\n",
        "        ax.clear()\n",
        "        ax.set_xlim(final_2d[:, 0].min() - 1, final_2d[:, 0].max() + 1)\n",
        "        ax.set_ylim(final_2d[:, 1].min() - 1, final_2d[:, 1].max() + 1)\n",
        "        ax.grid(True)\n",
        "        ax.set_title(f\"Field Evolution for: {sample_text[:50]}...\")\n",
        "        time_text.set_text('')\n",
        "        return []\n",
        "    \n",
        "    def animate(i):\n",
        "        nonlocal query_point, attractor_points\n",
        "        \n",
        "        ax.clear()\n",
        "        \n",
        "        # Get positions at step i\n",
        "        positions = field_history[i].cpu().numpy()\n",
        "        \n",
        "        # Apply the same transformation to keep positions consistent\n",
        "        positions_2d = final_2d\n",
        "        \n",
        "        # Query point (first point)\n",
        "        query_x, query_y = positions_2d[0, 0], positions_2d[0, 1]\n",
        "        query_point = ax.scatter(query_x, query_y, color='red', s=100, marker='*', label='Query')\n",
        "        \n",
        "        # Attractor points (remaining points)\n",
        "        attractor_x, attractor_y = positions_2d[1:, 0], positions_2d[1:, 1]\n",
        "        attractor_points = ax.scatter(attractor_x, attractor_y, color='blue', s=50, alpha=0.7, label='Attractors')\n",
        "        \n",
        "        # Add labels to attractors\n",
        "        for j in range(len(positions_2d) - 1):\n",
        "            ax.text(positions_2d[j+1, 0], positions_2d[j+1, 1], f\"A{j}\", fontsize=8)\n",
        "        \n",
        "        # Add step information\n",
        "        time_text.set_text(f'Step: {i}/{num_steps-1}')\n",
        "        \n",
        "        ax.set_xlim(final_2d[:, 0].min() - 1, final_2d[:, 0].max() + 1)\n",
        "        ax.set_ylim(final_2d[:, 1].min() - 1, final_2d[:, 1].max() + 1)\n",
        "        ax.grid(True)\n",
        "        ax.set_title(f\"Field Evolution for: {sample_text[:50]}...\")\n",
        "        ax.legend()\n",
        "        \n",
        "        return [query_point, attractor_points, time_text]\n",
        "    \n",
        "    # Create animation\n",
        "    ani = FuncAnimation(fig, animate, frames=range(num_steps), init_func=init, blit=False, interval=300)\n",
        "    \n",
        "    # Save animation\n",
        "    ani.save('/content/field_evolution_optimized.gif', writer='pillow', fps=3)\n",
        "    \n",
        "    # Display animation\n",
        "    from IPython.display import Image\n",
        "    display(Image(filename='/content/field_evolution_optimized.gif'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test visualization with sample texts\n",
        "sample_texts = [\n",
        "    \"The recipe was simple, requiring only five common ingredients.\",\n",
        "    \"After receiving the terminal diagnosis, he sank into despair, unable to see any future.\",\n",
        "    \"The building was fully engulfed, flames burning bright against the night sky.\"\n",
        "]\n",
        "\n",
        "visualize_field_static(model, sample_texts, \"Static Field Visualization (Optimized Model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Visualize field evolution for a single sample\n",
        "sample_text = \"Her creativity allowed her to see solutions that others missed.\"\n",
        "visualize_field_evolution(model, sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept Navigation Visualization\n",
        "\n",
        "Visualize how the model navigates between concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def find_closest_concept(model, text, dataset):\n",
        "    \"\"\"Find the closest concept to the given text.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Get embedding for the input text\n",
        "    with torch.no_grad():\n",
        "        input_embedding = model.encode_text([text])[0]\n",
        "    \n",
        "    # Get embeddings for all concepts in the dataset\n",
        "    concept_texts = []\n",
        "    concept_labels = []\n",
        "    \n",
        "    # Extract all unique concept texts from dataset\n",
        "    seen_texts = set()\n",
        "    \n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        \n",
        "        # Source concept\n",
        "        if sample['source_text'] not in seen_texts:\n",
        "            concept_texts.append(sample['source_text'])\n",
        "            concept_labels.append(f\"{sample['source_space']}/{sample['source_concept']}\")\n",
        "            seen_texts.add(sample['source_text'])\n",
        "        \n",
        "        # Target concept\n",
        "        if sample['target_text'] not in seen_texts:\n",
        "            concept_texts.append(sample['target_text'])\n",
        "            concept_labels.append(f\"{sample['target_space']}/{sample['target_concept']}\")\n",
        "            seen_texts.add(sample['target_text'])\n",
        "    \n",
        "    # Get embeddings for all concepts\n",
        "    concept_embeddings = []\n",
        "    batch_size = 64  # Increased from 32\n",
        "    \n",
        "    for i in range(0, len(concept_texts), batch_size):\n",
        "        batch_texts = concept_texts[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "            batch_embeddings = model.encode_text(batch_texts)\n",
        "            concept_embeddings.append(batch_embeddings)\n",
        "    \n",
        "    concept_embeddings = torch.cat(concept_embeddings, dim=0)\n",
        "    \n",
        "    # Compute cosine similarities\n",
        "    input_embedding = input_embedding.unsqueeze(0)  # Add batch dimension\n",
        "    similarities = F.cosine_similarity(input_embedding, concept_embeddings)\n",
        "    \n",
        "    # Get top 5 closest concepts\n",
        "    top_indices = similarities.argsort(descending=True)[:5]\n",
        "    \n",
        "    # Return results\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            'label': concept_labels[idx],\n",
        "            'text': concept_texts[idx],\n",
        "            'similarity': similarities[idx].item()\n",
        "        })\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Test concept navigation\n",
        "test_input = \"The temperature was very cold, almost freezing.\"\n",
        "closest_concepts = find_closest_concept(model, test_input, dataset)\n",
        "\n",
        "print(f\"Input: {test_input}\\n\")\n",
        "print(\"Closest concepts:\")\n",
        "for i, concept in enumerate(closest_concepts):\n",
        "    print(f\"{i+1}. {concept['label']} (similarity: {concept['similarity']:.4f})\")\n",
        "    print(f\"   Text: {concept['text']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chatbot Interface\n",
        "\n",
        "Create a simple chatbot interface to interact with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "class LaminetChatbot:\n",
        "    \"\"\"Simple chatbot interface for the Laminet model.\"\"\"\n",
        "    def __init__(self, model, dataset, memory_size=5):\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = []  # Store recent interactions\n",
        "    \n",
        "    def chat(self, user_input):\n",
        "        \"\"\"Process user input and generate a response.\"\"\"\n",
        "        # Add user input to memory\n",
        "        self.memory.append({'role': 'user', 'text': user_input})\n",
        "        \n",
        "        # Find closest concepts\n",
        "        closest_concepts = find_closest_concept(self.model, user_input, self.dataset)\n",
        "        \n",
        "        # Get the closest concept\n",
        "        top_concept = closest_concepts[0]\n",
        "        \n",
        "        # Find samples that have this concept as source\n",
        "        concept_label = top_concept['label']\n",
        "        space, concept = concept_label.split('/')\n",
        "        \n",
        "        # Look for samples with the matched concept as source\n",
        "        matching_samples = []\n",
        "        for i in range(len(self.dataset)):\n",
        "            sample = self.dataset[i]\n",
        "            if sample['source_space'] == space and sample['source_concept'] == concept:\n",
        "                matching_samples.append(sample)\n",
        "        \n",
        "        # If no matching samples, use the closest concept's text as response\n",
        "        if not matching_samples:\n",
        "            response = top_concept['text']\n",
        "        else:\n",
        "            # Use a random matching sample's target text as response\n",
        "            selected_sample = random.choice(matching_samples)\n",
        "            response = selected_sample['target_text']\n",
        "        \n",
        "        # Add response to memory\n",
        "        self.memory.append({'role': 'assistant', 'text': response})\n",
        "        \n",
        "        # Trim memory if too large\n",
        "        if len(self.memory) > self.memory_size * 2:\n",
        "            self.memory = self.memory[-self.memory_size * 2:]\n",
        "        \n",
        "        return response, closest_concepts\n",
        "    \n",
        "    def get_conversation_history(self):\n",
        "        \"\"\"Get the conversation history.\"\"\"\n",
        "        return self.memory\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset the conversation history.\"\"\"\n",
        "        self.memory = []\n",
        "        return \"Conversation history cleared.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize chatbot\n",
        "chatbot = LaminetChatbot(model, dataset)\n",
        "\n",
        "# Interactive chat interface using IPython widgets\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Chat history display\n",
        "chat_history = widgets.HTML(value=\"\")\n",
        "\n",
        "# Text input for user\n",
        "text_input = widgets.Text(\n",
        "    placeholder='Type your message here...',\n",
        "    description='You:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "# Send button\n",
        "send_button = widgets.Button(\n",
        "    description='Send',\n",
        "    button_style='primary',\n",
        "    layout=widgets.Layout(width='15%')\n",
        ")\n",
        "\n",
        "# Reset button\n",
        "reset_button = widgets.Button(\n",
        "    description='Reset Chat',\n",
        "    button_style='danger',\n",
        "    layout=widgets.Layout(width='15%')\n",
        ")\n",
        "\n",
        "def update_chat_display():\n",
        "    \"\"\"Update the chat display with current conversation history.\"\"\"\n",
        "    history = chatbot.get_conversation_history()\n",
        "    html = \"\"\"\n",
        "    <style>\n",
        "        .chat-container { font-family: Arial, sans-serif; }\n",
        "        .user-message { background-color: #e6f7ff; padding: 10px; border-radius: 10px; margin: 5px 0; text-align: right; }\n",
        "        .assistant-message { background-color: #f1f1f1; padding: 10px; border-radius: 10px; margin: 5px 0; }\n",
        "    </style>\n",
        "    <div class=\"chat-container\">\n",
        "    \"\"\"\n",
        "    \n",
        "    for message in history:\n",
        "        if message['role'] == 'user':\n",
        "            html += f\"<div class='user-message'><strong>You:</strong> {message['text']}</div>\"\n",
        "        else:\n",
        "            html += f\"<div class='assistant-message'><strong>Laminet:</strong> {message['text']}</div>\"\n",
        "    \n",
        "    html += \"</div>\"\n",
        "    chat_history.value = html\n",
        "\n",
        "def on_send_clicked(b):\n",
        "    \"\"\"Handle send button click.\"\"\"\n",
        "    user_input = text_input.value\n",
        "    if not user_input.strip():\n",
        "        return\n",
        "    \n",
        "    # Clear input field\n",
        "    text_input.value = \"\"\n",
        "    \n",
        "    # Process input and get response\n",
        "    response, concepts = chatbot.chat(user_input)\n",
        "    \n",
        "    # Update chat display\n",
        "    update_chat_display()\n",
        "    \n",
        "    # Print debugging info about closest concepts\n",
        "    print(\"Closest concepts:\")\n",
        "    for i, concept in enumerate(concepts[:3]):\n",
        "        print(f\"{i+1}. {concept['label']} (similarity: {concept['similarity']:.4f})\")\n",
        "\n",
        "def on_reset_clicked(b):\n",
        "    \"\"\"Handle reset button click.\"\"\"\n",
        "    chatbot.reset()\n",
        "    update_chat_display()\n",
        "    print(\"Chat history cleared.\")\n",
        "\n",
        "# Add event handlers\n",
        "send_button.on_click(on_send_clicked)\n",
        "reset_button.on_click(on_reset_clicked)\n",
        "\n",
        "# Handle Enter key in text input\n",
        "def on_enter(sender):\n",
        "    on_send_clicked(None)\n",
        "\n",
        "text_input.on_submit(on_enter)\n",
        "\n",
        "# Layout\n",
        "input_box = widgets.HBox([text_input, send_button])\n",
        "chat_interface = widgets.VBox([chat_history, input_box, reset_button])\n",
        "\n",
        "# Display interface\n",
        "display(chat_interface)\n",
        "\n",
        "# Initial update\n",
        "update_chat_display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This optimized notebook has demonstrated how to:\n",
        "\n",
        "1. Build a streamlined Laminet model with reduced complexity\n",
        "2. Train it efficiently using mixed precision training\n",
        "3. Achieve similar functionality in 30-60 minutes (instead of 3-5 hours)\n",
        "4. Visualize the semantic field and create a working chatbot\n",
        "\n",
        "The optimizations include:\n",
        "- Smaller field dimension (64 vs 128)\n",
        "- Fewer attractor points (20 vs 50)\n",
        "- Fewer evolution steps (5 vs 10)\n",
        "- Mixed precision training\n",
        "- Optimized field evolution calculations\n",
        "- Larger batch size (64 vs 32)\n",
        "- Fewer training epochs (8 vs 20)\n",
        "\n",
        "While these changes reduce training time significantly, the model still provides meaningful concept navigation and semantic field evolution as described in the Lamina Networks whitepaper."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}